{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercício 10 - Redes neuronais"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from si.neural_networks.nn import NN\n",
    "from si.neural_networks.layers import Dense, SigmoidActivation, SoftMaxActivation, ReLUActivation, LinearActivation\n",
    "from si.data.dataset_module import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "10.3. Problema binário, modelo com SigmoidActivation como layer de ativação"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Gerar training dataset\n",
    "x_3 = np.random.randn(100, 32) # 100 samples com 32 features de floats\n",
    "y_3 = np.random.randint(0, 2, size=(100, 1)) # labels para as 100 samples com 0 e 1 random\n",
    "\n",
    "dataset_3 = Dataset(x_3, y_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "l3_1 = Dense(input_size=32, output_size=32)\n",
    "l3_2 = Dense(input_size=32, output_size=16)\n",
    "l3_3 = Dense(input_size=16, output_size=1)\n",
    "\n",
    "l3_1sg = SigmoidActivation()\n",
    "l3_2sg = SigmoidActivation()\n",
    "l3_3sg = SigmoidActivation()\n",
    "\n",
    "# layers\n",
    "layers = [l3_1, l3_1sg, l3_2, l3_2sg, l3_3, l3_3sg]\n",
    "\n",
    "# NN\n",
    "nn_model3 = NN(layers=layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.49416414],\n       [0.49416255],\n       [0.49416062],\n       [0.49415955],\n       [0.49415875],\n       [0.49416032],\n       [0.49416048],\n       [0.49416253],\n       [0.49415971],\n       [0.49416274],\n       [0.4941634 ],\n       [0.49416269],\n       [0.49415846],\n       [0.49415949],\n       [0.49416269],\n       [0.49416035],\n       [0.49416519],\n       [0.49416326],\n       [0.4941634 ],\n       [0.49416126],\n       [0.49416246],\n       [0.49416477],\n       [0.4941606 ],\n       [0.49415876],\n       [0.49415936],\n       [0.49416414],\n       [0.49416061],\n       [0.49416131],\n       [0.49416096],\n       [0.49416095],\n       [0.49416457],\n       [0.49416487],\n       [0.4941638 ],\n       [0.49416164],\n       [0.4941605 ],\n       [0.494163  ],\n       [0.49416108],\n       [0.49416025],\n       [0.49415778],\n       [0.4941636 ],\n       [0.49415942],\n       [0.49416231],\n       [0.49416169],\n       [0.49415817],\n       [0.49416203],\n       [0.49416025],\n       [0.49416072],\n       [0.49416317],\n       [0.49415927],\n       [0.49416156],\n       [0.49416184],\n       [0.4941606 ],\n       [0.49416121],\n       [0.49415952],\n       [0.4941638 ],\n       [0.49416283],\n       [0.49416173],\n       [0.49415781],\n       [0.49416394],\n       [0.49416294],\n       [0.49416101],\n       [0.49416043],\n       [0.49416191],\n       [0.49416237],\n       [0.49416318],\n       [0.49416402],\n       [0.49416267],\n       [0.49416305],\n       [0.49416384],\n       [0.49415912],\n       [0.49416185],\n       [0.49416359],\n       [0.49416288],\n       [0.49416029],\n       [0.49416398],\n       [0.49415988],\n       [0.49416206],\n       [0.4941627 ],\n       [0.4941586 ],\n       [0.49416164],\n       [0.49416262],\n       [0.49416252],\n       [0.49415792],\n       [0.4941604 ],\n       [0.49416321],\n       [0.494163  ],\n       [0.49416005],\n       [0.4941615 ],\n       [0.49416075],\n       [0.49416201],\n       [0.49416172],\n       [0.49416072],\n       [0.4941627 ],\n       [0.49416144],\n       [0.49416022],\n       [0.49416187],\n       [0.49415896],\n       [0.49415949],\n       [0.49416131],\n       [0.49416343]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model3.predict(dataset=dataset_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10.4. Problema multiclass com 3 classes, modelo com SigmoidActivation como layer de ativação e SoftMaxActivation como última layer de ativação"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Gerar training dataset\n",
    "x_4 = np.random.randn(100, 32) # 100 samples com 32 features de floats\n",
    "y_4 = np.random.randint(0, 3, size=(100, 1)) # labels para as 100 samples com 3 classes\n",
    "\n",
    "dataset_4 = Dataset(x_4, y_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "l4_1 = Dense(input_size=32, output_size=32)\n",
    "l4_2 = Dense(input_size=32, output_size=16)\n",
    "l4_3 = Dense(input_size=16, output_size=3)\n",
    "\n",
    "l4_1sg = SigmoidActivation()\n",
    "l4_2sg = SigmoidActivation()\n",
    "l4_3sma = SoftMaxActivation()\n",
    "\n",
    "# layers\n",
    "layers = [l4_1, l4_1sg, l4_2, l4_2sg, l4_3, l4_3sma]\n",
    "\n",
    "# NN\n",
    "nn_model4 = NN(layers=layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.3326322 , 0.33956793, 0.32779987],\n       [0.33262828, 0.33956961, 0.32780212],\n       [0.33262791, 0.33956526, 0.32780683],\n       [0.33263046, 0.33956506, 0.32780449],\n       [0.33263453, 0.33956678, 0.32779869],\n       [0.33263378, 0.33956792, 0.3277983 ],\n       [0.33262681, 0.3395692 , 0.32780398],\n       [0.33263484, 0.33956702, 0.32779814],\n       [0.33262525, 0.33956855, 0.3278062 ],\n       [0.33262832, 0.33956746, 0.32780422],\n       [0.33262934, 0.33956894, 0.32780172],\n       [0.33262873, 0.33956922, 0.32780205],\n       [0.33262996, 0.33956777, 0.32780227],\n       [0.33262622, 0.33956826, 0.32780552],\n       [0.33263154, 0.33956298, 0.32780548],\n       [0.33263372, 0.33956621, 0.32780007],\n       [0.33263399, 0.33956767, 0.32779834],\n       [0.33263023, 0.33956908, 0.32780068],\n       [0.33262609, 0.33956426, 0.32780965],\n       [0.33263143, 0.33956655, 0.32780202],\n       [0.33262973, 0.33956781, 0.32780246],\n       [0.33263585, 0.33956596, 0.32779819],\n       [0.33263292, 0.33956494, 0.32780214],\n       [0.33262971, 0.33956731, 0.32780298],\n       [0.33262864, 0.33956765, 0.3278037 ],\n       [0.33262938, 0.33956938, 0.32780123],\n       [0.33263345, 0.33956657, 0.32779999],\n       [0.33263022, 0.33956859, 0.32780119],\n       [0.33263166, 0.3395675 , 0.32780085],\n       [0.33263012, 0.33956853, 0.32780135],\n       [0.33263416, 0.33956796, 0.32779789],\n       [0.33262409, 0.33956951, 0.3278064 ],\n       [0.33262821, 0.33956742, 0.32780437],\n       [0.33262658, 0.33956871, 0.32780471],\n       [0.33263495, 0.33956642, 0.32779863],\n       [0.33262988, 0.33956666, 0.32780346],\n       [0.33262939, 0.33956608, 0.32780453],\n       [0.33262436, 0.33956658, 0.32780906],\n       [0.3326344 , 0.33956683, 0.32779878],\n       [0.33262695, 0.33957057, 0.32780248],\n       [0.33262546, 0.33956887, 0.32780567],\n       [0.33263753, 0.33956508, 0.32779739],\n       [0.33262744, 0.33956644, 0.32780612],\n       [0.33262968, 0.33956659, 0.32780374],\n       [0.33263371, 0.33956545, 0.32780084],\n       [0.33263197, 0.33956629, 0.32780174],\n       [0.33262465, 0.33956694, 0.32780841],\n       [0.33263264, 0.33956776, 0.3277996 ],\n       [0.33262624, 0.33956852, 0.32780524],\n       [0.33263156, 0.33956806, 0.32780038],\n       [0.33262834, 0.33956675, 0.32780491],\n       [0.33263123, 0.33956683, 0.32780194],\n       [0.33263275, 0.33956639, 0.32780086],\n       [0.33263095, 0.33956816, 0.32780089],\n       [0.33263587, 0.33956648, 0.32779765],\n       [0.33262635, 0.33956843, 0.32780522],\n       [0.3326279 , 0.33956787, 0.32780422],\n       [0.33262274, 0.33956851, 0.32780874],\n       [0.33263639, 0.33956455, 0.32779906],\n       [0.33263045, 0.33956519, 0.32780436],\n       [0.33263016, 0.33956833, 0.32780151],\n       [0.33263146, 0.3395676 , 0.32780094],\n       [0.33263474, 0.33956654, 0.32779873],\n       [0.33263346, 0.33956568, 0.32780086],\n       [0.3326321 , 0.33956748, 0.32780042],\n       [0.33262682, 0.3395651 , 0.32780808],\n       [0.33262765, 0.33956662, 0.32780573],\n       [0.3326333 , 0.33956795, 0.32779875],\n       [0.33262673, 0.33956628, 0.32780699],\n       [0.33262422, 0.33956901, 0.32780677],\n       [0.33262906, 0.33956646, 0.32780448],\n       [0.33263478, 0.33956604, 0.32779917],\n       [0.33262789, 0.33956752, 0.32780458],\n       [0.33262984, 0.3395663 , 0.32780386],\n       [0.3326283 , 0.3395687 , 0.327803  ],\n       [0.3326249 , 0.33956886, 0.32780625],\n       [0.33262639, 0.33956845, 0.32780516],\n       [0.33262984, 0.33956743, 0.32780274],\n       [0.33262909, 0.33956687, 0.32780404],\n       [0.33263041, 0.3395664 , 0.32780319],\n       [0.33262474, 0.33956863, 0.32780663],\n       [0.33263018, 0.33956794, 0.32780187],\n       [0.33262511, 0.33956923, 0.32780566],\n       [0.33263322, 0.33956542, 0.32780136],\n       [0.33263254, 0.33956479, 0.32780266],\n       [0.33263304, 0.33956679, 0.32780017],\n       [0.33262712, 0.33956649, 0.32780639],\n       [0.33263222, 0.33956618, 0.3278016 ],\n       [0.33263267, 0.3395682 , 0.32779913],\n       [0.33262232, 0.33956863, 0.32780905],\n       [0.3326242 , 0.33956832, 0.32780748],\n       [0.33262516, 0.3395678 , 0.32780704],\n       [0.33263087, 0.33956366, 0.32780547],\n       [0.33262875, 0.33956744, 0.3278038 ],\n       [0.33262363, 0.33957002, 0.32780635],\n       [0.33263006, 0.33956721, 0.32780273],\n       [0.33262477, 0.33956929, 0.32780593],\n       [0.33262804, 0.33956764, 0.32780432],\n       [0.33263159, 0.33956599, 0.32780242],\n       [0.33263173, 0.33956684, 0.32780143]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model4.predict(dataset=dataset_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10.5. Problema de regressão, modelo com 3 Dense layers, ReLU como layer de ativação e o modelo deve acabar com uma ativação linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Gerar training dataset\n",
    "x_5 = np.random.randn(100, 32) # 100 samples com 32 features de floats\n",
    "y_5 = np.random.randn(100, 1) # labels de floats para as 100 samples\n",
    "\n",
    "dataset_5 = Dataset(x_5, y_5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "l5_1 = Dense(input_size=32, output_size=32)\n",
    "l5_2 = Dense(input_size=32, output_size=16)\n",
    "l5_3 = Dense(input_size=16, output_size=1)\n",
    "\n",
    "l5_1relu = ReLUActivation()\n",
    "l5_2relu = ReLUActivation()\n",
    "l5_3la = LinearActivation()\n",
    "\n",
    "# layers\n",
    "layers = [l5_1, l5_1relu, l5_2, l5_2relu, l5_3, l5_3la]\n",
    "\n",
    "# NN\n",
    "nn_model5 = NN(layers=layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m nn_model5\u001B[38;5;241m.\u001B[39mpredict(dataset\u001B[38;5;241m=\u001B[39mdataset_5)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mbioinf-sib\\src\\si\\neural_networks\\nn.py:118\u001B[0m, in \u001B[0;36mNN.predict\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m# forward propagation\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 118\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "nn_model5.predict(dataset=dataset_5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}